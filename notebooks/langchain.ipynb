{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8d9bf7",
   "metadata": {},
   "source": "# ðŸ”— LangChain + LLM Abstractions\n\nIn this notebook we'll move from the raw OpenAI SDK to **LangChain**, which provides higher-level abstractions to make LLM use easier.\n\nWe'll cover:\n1. Setting up `ChatOpenAI`.  \n2. Prompt templates & the **LangChain Expression Language (LCEL)**.  \n3. Varying parameters like `temperature` and `top_p`.  \n4. **Streaming** responses with callbacks.  \n5. **Batching & parallel calls** with `.batch()`.  \n6. Getting **structured outputs** with Pydantic models.  \n7. Swapping configs at runtime (`.bind`, `.with_config`).  \n8. (Optional) Using Azure OpenAI through LangChain.\n\nBy the end, you'll see how LangChain **simplifies interaction, chaining, and integration** with LLMs.\n\n---\n\n## 1. Basic LLM Setup\n\nInitialize ChatOpenAI with model configuration and API credentials. This is your primary interface for interacting with OpenAI's models through LangChain."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c722d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,   # same knobs as OpenAI\n",
    "    # top_p=0.9,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecebeabc",
   "metadata": {},
   "source": "## 2. Direct LLM Invocation\n\nCall the LLM directly using `.invoke()` method. You can pass simple strings or structured message lists with system and user roles."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76d8891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! Here are two quick tips for learning Python:\\n\\n1. **Practice Regularly**: Consistency is key when learning a programming language. Set aside time each day or week to write code, work on small projects, or solve coding challenges on platforms like LeetCode or HackerRank. This will help reinforce your understanding and improve your problem-solving skills.\\n\\n2. **Utilize Online Resources**: Take advantage of the wealth of online resources available, such as tutorials, documentation, and forums. Websites like Codecademy, freeCodeCamp, and the official Python documentation are great places to start. Engaging with communities on platforms like Stack Overflow or Reddit can also provide support and insights from other learners and experienced developers.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 16, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CeP7Ok6q4iG8r7NjulpKUYnB3FU8M', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--06363e7c-1e3b-48bf-bfa2-0491545e10ed-0', usage_metadata={'input_tokens': 16, 'output_tokens': 145, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Give me two quick tips for learning Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13044ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Please provide the text you'd like me to summarize.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 30, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CPWzwNRVCPYwmaoVU2kGYICEyNFKn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3a0fab8f-394f-436a-bca0-43495c94132b-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful, concise assistant.\"),\n",
    "        (\"user\", \"Summarize this in 2 bullets:\\n\\n{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e97f9",
   "metadata": {},
   "source": "## 3. Prompt Templates\n\nCreate reusable prompt templates with variables using `ChatPromptTemplate`. This allows you to define prompts once and reuse them with different inputs, making your code more maintainable and flexible."
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, concise assistant.\"),\n",
    "    (\"user\", \"Summarize this in 2 bullets:\\n\\n{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61197895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful, concise assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Summarize this in 2 bullets:\\n\\n{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d77e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example = \"Transformers use attention to weigh context; embeddings turn tokens into vectors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c8ec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful, concise assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Summarize this in 2 bullets:\\n\\nTransformers use attention to weigh context; embeddings turn tokens into vectors.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_rendered = prompt.invoke({\"text\": text_example})\n",
    "prompt_rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4524de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt_rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39404ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize context in processing information.\n",
      "- Embeddings convert tokens into vector representations for effective model input.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cb98571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize context in processing information.\n",
      "- Embeddings convert tokens into vector representations for effective model input.\n"
     ]
    }
   ],
   "source": [
    "content = StrOutputParser().invoke(response)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bp51ayagcq",
   "source": "## 4. LangChain Expression Language (LCEL)\n\nChain components together using the pipe operator `|`. LCEL allows you to build powerful pipelines that flow data from prompts â†’ LLMs â†’ output parsers in a clean, readable syntax.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5667bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b0d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- Transformers utilize attention mechanisms to prioritize context in processing data.\\n- Embeddings convert tokens into vector representations for better understanding and manipulation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 42, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CPXB4Vdldpsa4adgGlhvNSyJa8PHd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--eb575178-fda2-4264-ab18-1127973de7d1-0', usage_metadata={'input_tokens': 42, 'output_tokens': 26, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1.invoke({\"text\": text_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01b7b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"text\": text_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca2f7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize context in processing data.\n",
      "- Embeddings convert tokens into vector representations for better model understanding.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50pxzt8f2qk",
   "source": "## 5. Streaming Responses\n\nStream LLM responses token-by-token using callbacks. This provides a better user experience by showing output as it's generated, rather than waiting for the complete response.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d191f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentle drops descend,  \n",
      "Whispers dance on window panes,  \n",
      "Nature's soft embrace.  \n",
      "Puddles mirror gray skies,  \n",
      "Life awakens anew.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class PrintHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        print(token, end=\"\")\n",
    "\n",
    "stream_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, streaming=True, callbacks=[PrintHandler()])\n",
    "(stream_llm | StrOutputParser()).invoke(\"Stream a 5-sentence haiku about rain.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18g226hsg9",
   "source": "## 6. Batch Processing\n\nProcess multiple inputs in parallel using `.batch()`. This is efficient for handling multiple questions or requests simultaneously, saving time compared to sequential processing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99161421",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca4ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is overfitting?\",\n",
    "    \"Explain dropout in one line.\",\n",
    "    \"Contrast precision vs recall briefly.\"\n",
    "]\n",
    "# Runnable.batch for parallel execution\n",
    "answers = chain.batch(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b14a61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is overfitting?\n",
      "A: Overfitting is a common problem in machine learning and statistical modeling where a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This results in a model that performs exceptionally well on the training dataset but poorly on unseen data or test datasets. \n",
      "\n",
      "Key characteristics of overfitting include:\n",
      "\n",
      "1. **High Training Accuracy, Low Test Accuracy**: The model shows excellent performance on the training data but fails to generalize to new, unseen data.\n",
      "\n",
      "2. **Complex Models**: Overfitting often occurs with overly complex models that have too many parameters relative to the amount of training data. Such models can fit the training data very closely, including its noise.\n",
      "\n",
      "3. **Insufficient Data**: When the amount of training data is limited, models may overfit because they do not have enough examples to learn the true underlying distribution.\n",
      "\n",
      "To mitigate overfitting, several techniques can be employed:\n",
      "\n",
      "- **Cross-Validation**: Using techniques like k-fold cross-validation helps ensure that the model is evaluated on multiple subsets of the data.\n",
      "  \n",
      "- **Regularization**: Techniques such as L1 (Lasso) and L2 (Ridge) regularization add a penalty for complexity to the loss function, discouraging overly complex models.\n",
      "\n",
      "- **Pruning**: In decision trees, pruning can help reduce the size of the tree to prevent it from fitting noise.\n",
      "\n",
      "- **Early Stopping**: Monitoring the model's performance on a validation set and stopping training when performance begins to degrade can help prevent overfitting.\n",
      "\n",
      "- **Data Augmentation**: Increasing the amount of training data through techniques like augmentation can help the model generalize better.\n",
      "\n",
      "By addressing overfitting, models can achieve better generalization, leading to improved performance on new data.\n",
      "\n",
      "Q: Explain dropout in one line.\n",
      "A: Dropout is a regularization technique in neural networks that randomly sets a fraction of the neurons to zero during training to prevent overfitting.\n",
      "\n",
      "Q: Contrast precision vs recall briefly.\n",
      "A: Precision and recall are two important metrics used to evaluate the performance of classification models, particularly in scenarios where the classes are imbalanced.\n",
      "\n",
      "- **Precision** measures the accuracy of the positive predictions made by the model. It is defined as the ratio of true positive predictions to the total number of positive predictions (true positives + false positives). High precision indicates that when the model predicts a positive class, it is likely to be correct.\n",
      "\n",
      "  \\[\n",
      "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
      "  \\]\n",
      "\n",
      "- **Recall** (also known as sensitivity or true positive rate) measures the model's ability to identify all relevant instances. It is defined as the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives). High recall indicates that the model captures most of the positive cases.\n",
      "\n",
      "  \\[\n",
      "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
      "  \\]\n",
      "\n",
      "In summary, precision focuses on the quality of the positive predictions, while recall emphasizes the model's ability to find all relevant positive instances. Balancing both metrics is often necessary, especially in applications where false positives and false negatives have different costs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q, a in zip(questions, answers):\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7hc3yh754io",
   "source": "## 7. Structured Outputs with Pydantic\n\nGet type-safe, structured responses from LLMs using Pydantic models. Instead of parsing raw text, LangChain automatically converts LLM outputs into Python objects with defined schemas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23501a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flashcard(term='Positional Encoding', definition='A technique used in Transformers to inject information about the position of tokens in a sequence, allowing the model to understand the order of the input data.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Flashcard(BaseModel):\n",
    "    term: str = Field(..., description=\"Short term\")\n",
    "    definition: str = Field(..., description=\"One-sentence definition\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Flashcard)  # Let LC coax JSONâ†’model\n",
    "card = structured_llm.invoke(\"Create a flashcard about 'positional encoding' in Transformers.\")\n",
    "card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff2a4b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positional Encoding'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.term"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}