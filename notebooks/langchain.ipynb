{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8d9bf7",
   "metadata": {},
   "source": [
    "# ðŸ”— LangChain + LLM Abstractions\n",
    "\n",
    "In this notebook weâ€™ll move from the raw OpenAI SDK to **LangChain**, which provides higher-level abstractions to make LLM use easier.\n",
    "\n",
    "Weâ€™ll cover:\n",
    "1. Setting up `ChatOpenAI`.  \n",
    "2. Prompt templates & the **LangChain Expression Language (LCEL)**.  \n",
    "3. Varying parameters like `temperature` and `top_p`.  \n",
    "4. **Streaming** responses with callbacks.  \n",
    "5. **Batching & parallel calls** with `.batch()`.  \n",
    "6. Getting **structured outputs** with Pydantic models.  \n",
    "7. Swapping configs at runtime (`.bind`, `.with_config`).  \n",
    "8. (Optional) Using Azure OpenAI through LangChain.\n",
    "\n",
    "By the end, youâ€™ll see how LangChain **simplifies interaction, chaining, and integration** with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c722d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,   # same knobs as OpenAI\n",
    "    # top_p=0.9,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76d8891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Here are two quick tips for learning Python:\\n\\n1. **Practice Regularly**: Consistency is key when learning a programming language. Set aside time each day or week to practice coding. Work on small projects, solve coding challenges on platforms like LeetCode or HackerRank, and try to build something that interests you.\\n\\n2. **Utilize Online Resources**: Take advantage of the wealth of online resources available. Websites like Codecademy, freeCodeCamp, and Coursera offer interactive courses, while documentation and tutorials on the official Python website can provide in-depth knowledge. Engaging with communities on forums like Stack Overflow or Reddit can also help you get answers to your questions and learn from others' experiences.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 16, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CPWyLtZvfGpTRSaaNn8TddoDAKGek', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--91dd1b89-dad8-458b-b814-e373d37121e6-0', usage_metadata={'input_tokens': 16, 'output_tokens': 145, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Give me two quick tips for learning Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13044ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Please provide the text you'd like me to summarize.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 30, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CPWzwNRVCPYwmaoVU2kGYICEyNFKn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3a0fab8f-394f-436a-bca0-43495c94132b-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful, concise assistant.\"),\n",
    "        (\"user\", \"Summarize this in 2 bullets:\\n\\n{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, concise assistant.\"),\n",
    "    (\"user\", \"Summarize this in 2 bullets:\\n\\n{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61197895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful, concise assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Summarize this in 2 bullets:\\n\\n{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d77e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example = \"Transformers use attention to weigh context; embeddings turn tokens into vectors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c8ec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful, concise assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Summarize this in 2 bullets:\\n\\nTransformers use attention to weigh context; embeddings turn tokens into vectors.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_rendered = prompt.invoke({\"text\": text_example})\n",
    "prompt_rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4524de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt_rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39404ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize context in processing information.\n",
      "- Embeddings convert tokens into vector representations for effective model input.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cb98571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize context in processing information.\n",
      "- Embeddings convert tokens into vector representations for effective model input.\n"
     ]
    }
   ],
   "source": [
    "content = StrOutputParser().invoke(response)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5667bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb7b0d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- Transformers utilize attention mechanisms to prioritize context in processing data.\\n- Embeddings convert tokens into vector representations for better understanding and manipulation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 42, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CPXB4Vdldpsa4adgGlhvNSyJa8PHd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--eb575178-fda2-4264-ab18-1127973de7d1-0', usage_metadata={'input_tokens': 42, 'output_tokens': 26, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\": text_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01b7b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"text\": text_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca2f7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize context in processing data.\n",
      "- Embeddings convert tokens into vector representations for better model understanding.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00d191f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whispers of the rain,  \n",
      "Dancing on the windowpanes,  \n",
      "Nature's soft embrace.  \n",
      "Puddles form like mirrors,  \n",
      "Reflecting gray skies' tears.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class PrintHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        print(token, end=\"\")\n",
    "\n",
    "stream_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, streaming=True, callbacks=[PrintHandler()])\n",
    "(stream_llm | StrOutputParser()).invoke(\"Stream a 5-sentence haiku about rain.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99161421",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eca4ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is overfitting?\",\n",
    "    \"Explain dropout in one line.\",\n",
    "    \"Contrast precision vs recall briefly.\"\n",
    "]\n",
    "# Runnable.batch for parallel execution\n",
    "answers = chain.batch(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b14a61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is overfitting?\n",
      "A: Overfitting is a common problem in machine learning and statistical modeling where a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This results in a model that performs exceptionally well on the training dataset but poorly on unseen data or test datasets. \n",
      "\n",
      "Key characteristics of overfitting include:\n",
      "\n",
      "1. **High Training Accuracy, Low Test Accuracy**: The model shows excellent performance on the training data but fails to generalize to new, unseen data.\n",
      "\n",
      "2. **Complex Models**: Overfitting often occurs with overly complex models that have too many parameters relative to the amount of training data. Such models can fit the training data very closely, including its noise.\n",
      "\n",
      "3. **Insufficient Data**: When the amount of training data is limited, models may overfit because they do not have enough examples to learn the true underlying distribution.\n",
      "\n",
      "To mitigate overfitting, several techniques can be employed:\n",
      "\n",
      "- **Cross-Validation**: Using techniques like k-fold cross-validation helps ensure that the model is evaluated on multiple subsets of the data.\n",
      "  \n",
      "- **Regularization**: Techniques such as L1 (Lasso) and L2 (Ridge) regularization add a penalty for complexity to the loss function, discouraging overly complex models.\n",
      "\n",
      "- **Pruning**: In decision trees, pruning can help reduce the size of the tree to prevent it from fitting noise.\n",
      "\n",
      "- **Early Stopping**: Monitoring the model's performance on a validation set and stopping training when performance begins to degrade can help prevent overfitting.\n",
      "\n",
      "- **Data Augmentation**: Increasing the amount of training data through techniques like augmentation can help the model generalize better.\n",
      "\n",
      "By addressing overfitting, models can achieve better generalization, leading to improved performance on new data.\n",
      "\n",
      "Q: Explain dropout in one line.\n",
      "A: Dropout is a regularization technique in neural networks that randomly sets a fraction of the neurons to zero during training to prevent overfitting.\n",
      "\n",
      "Q: Contrast precision vs recall briefly.\n",
      "A: Precision and recall are two important metrics used to evaluate the performance of classification models, particularly in scenarios where the classes are imbalanced.\n",
      "\n",
      "- **Precision** measures the accuracy of the positive predictions made by the model. It is defined as the ratio of true positive predictions to the total number of positive predictions (true positives + false positives). High precision indicates that when the model predicts a positive class, it is likely to be correct.\n",
      "\n",
      "  \\[\n",
      "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
      "  \\]\n",
      "\n",
      "- **Recall** (also known as sensitivity or true positive rate) measures the model's ability to identify all relevant instances. It is defined as the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives). High recall indicates that the model captures most of the positive cases.\n",
      "\n",
      "  \\[\n",
      "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
      "  \\]\n",
      "\n",
      "In summary, precision focuses on the quality of the positive predictions, while recall emphasizes the model's ability to find all relevant positive instances. Balancing both metrics is often necessary, especially in applications where false positives and false negatives have different costs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q, a in zip(questions, answers):\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23501a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flashcard(term='Positional Encoding', definition='A technique used in Transformers to inject information about the position of tokens in a sequence, allowing the model to understand the order of the input data.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Flashcard(BaseModel):\n",
    "    term: str = Field(..., description=\"Short term\")\n",
    "    definition: str = Field(..., description=\"One-sentence definition\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Flashcard)  # Let LC coax JSONâ†’model\n",
    "card = structured_llm.invoke(\"Create a flashcard about 'positional encoding' in Transformers.\")\n",
    "card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff2a4b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positional Encoding'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.term"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
