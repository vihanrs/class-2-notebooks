{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854488f2",
   "metadata": {},
   "source": "# üöÄ OpenAI Python SDK 101\n\nIn this notebook we'll learn how to interact with Large Language Models (LLMs) directly using the **OpenAI Python SDK**.  \nThis is the **first time** we're exploring API interactions, so we'll build up gradually:\n\n1. **Initialize** the client with your API key.  \n2. **Minimal call** to the API (Responses API).  \n3. Use **Chat Completions** for system + user roles.  \n4. Explore **temperature** (randomness) and **top_p** (nucleus sampling).  \n5. Add **system prompts** to guide behavior.  \n6. Try **streaming tokens** (like ChatGPT typing).  \n7. Get **JSON/structured outputs** with schemas.  \n8. Handle **errors, timeouts, and retries** gracefully.\n\nBy the end, you'll know how to **call an LLM safely and flexibly** using just the OpenAI SDK.\n\n---\n\n## 1. Client Initialization\n\nSet up the OpenAI client with your API key. This is your primary gateway to interact with OpenAI's models through their Python SDK."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844e5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf239a",
   "metadata": {},
   "source": [
    "### API key\n",
    "- Set your OpenAI API key as an environment variable:  \n",
    "  `export OPENAI_API_KEY=\"sk-...\"` (macOS/Linux) or `setx OPENAI_API_KEY \"sk-...\"` (Windows, new terminal required).  \n",
    "- In Colab: use `os.environ[\"OPENAI_API_KEY\"] = \"...\"` (for demos only).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e911ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Python SDK v1 style\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "td3qwx2vd3c",
   "source": "## 2. Responses API - Minimal Call\n\nUse the new Responses API for the simplest way to interact with OpenAI models. This is the recommended approach for new projects as it provides a cleaner, more streamlined interface.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9cd8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature controls the randomness of predictions by scaling logits, while top_p (nucleus sampling) selects from the top portion of the probability distribution, ensuring the sum of probabilities meets a specified threshold.\n"
     ]
    }
   ],
   "source": [
    "# Minimal \"Responses API\" call (recommended by OpenAI for new projects)\n",
    "# Docs: https://platform.openai.com/docs/guides/text  and Responses vs Chat Completions\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",  # choose any available text-capable model\n",
    "    input=\"In one sentence, explain the difference between temperature and top_p for sampling.\"\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fd52e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0c06274efbb9ef00006920963289bc81a187ec97bede0a20a3', created_at=1763743282.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_0c06274efbb9ef000069209634043c81a1a7708ebf4c2845bb', content=[ResponseOutputText(annotations=[], text='Temperature controls the randomness of predictions by scaling logits, while top_p (nucleus sampling) selects from the top portion of the probability distribution, ensuring the sum of probabilities meets a specified threshold.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=22, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=39, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=61), user=None, billing={'payer': 'developer'}, prompt_cache_retention=None, store=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pyuw78gk8pr",
   "source": "## 3. Chat Completions API\n\nUse the Chat Completions API to have more control over conversations. This allows you to specify different roles (system, user, assistant) for more sophisticated interactions and behavior guidance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff892c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Overfitting occurs when a machine learning model learns the training data too well, capturing noise and outliers, which hinders its performance on unseen data.\n",
      "\n",
      "- **Symptoms**: Indications of overfitting include a low training error coupled with a high validation or test error, suggesting the model is too complex for the given data.\n",
      "\n",
      "- **Prevention**: Techniques to prevent overfitting include using simpler models, regularization methods (e.g., L1 or L2 regularization), cross-validation, and techniques such as dropout in neural networks.\n"
     ]
    }
   ],
   "source": [
    "# Using Chat Completions (still widely used & supported)\n",
    "chat = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about overfitting.\"},\n",
    "    ],\n",
    ")\n",
    "print(chat.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01254fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CPVR1yklkREZ5BiYRWdVuBxDUU47x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='- **Definition**: Overfitting occurs when a machine learning model learns the training data too well, capturing noise and outliers, which hinders its performance on unseen data.\\n\\n- **Symptoms**: Indications of overfitting include a low training error coupled with a high validation or test error, suggesting the model is too complex for the given data.\\n\\n- **Prevention**: Techniques to prevent overfitting include using simpler models, regularization methods (e.g., L1 or L2 regularization), cross-validation, and techniques such as dropout in neural networks.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760195071, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=116, prompt_tokens=29, total_tokens=145, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hupgyjc4cdr",
   "source": "## 4. Temperature Control\n\nExperiment with the `temperature` parameter to control randomness in responses. Lower values (closer to 0) make outputs more deterministic and focused, while higher values (closer to 1) increase creativity and variation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5dddecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can reverse a string in Python using slicing. Here's a simple example:\n",
      "\n",
      "```python\n",
      "original_string = \"Hello, World!\"\n",
      "reversed_string = original_string[::-1]\n",
      "print(reversed_string)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "!dlroW ,olleH\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Python tutor who answers with short code examples.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Show how to reverse a string in Python.\"}\n",
    "]\n",
    "r = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, temperature=0)\n",
    "print(r.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63fd12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can reverse a string in Python using slicing. Here's a simple example:\n",
      "\n",
      "```python\n",
      "original_string = \"Hello, World!\"\n",
      "reversed_string = original_string[::-1]\n",
      "print(reversed_string)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "```\n",
      "!dlroW ,olleH\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "r = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, temperature=1)\n",
    "print(r.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cz9od1gls8",
   "source": "## 5. Streaming Responses\n\nStream tokens as they're generated instead of waiting for the complete response. This creates a more interactive experience similar to ChatGPT's typing effect, improving perceived responsiveness.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553e975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a cozy little town, there lived a adventurous tabby cat named Whiskers and a gentle golden retriever named Buddy. They lived in neighboring houses, separated only by a white picket fence. Though they were different in many ways, they shared one common trait: an insatiable curiosity about the world around them.\n",
      "\n",
      "One sunny afternoon, as the warm breeze rustled the leaves, Whiskers perched on the fence, gazing at the vast expanse of the backyard. ‚ÄúHey, Buddy!‚Äù she called, her green eyes sparkling with excitement. ‚ÄúLet‚Äôs go on an adventure!‚Äù\n",
      "\n",
      "Buddy looked up from his shady spot under the old oak tree. His tail wagged enthusiastically. ‚ÄúWhere to?‚Äù he asked, his voice a deep rumble of joy.\n",
      "\n",
      "‚ÄúLet‚Äôs explore the enchanted forest beyond the park!‚Äù Whiskers suggested, her whiskers twitching with anticipation. The forest was known for its shimmering streams and tall, whispering trees, but it also contained tales of hidden treasures and magical creatures.\n",
      "\n",
      "With a bark of agreement, Buddy bounded over to the fence, and together they slipped through a gap, ready for whatever awaited them.\n",
      "\n",
      "As they entered the forest, the sunlight filtered through the leaves, casting dappled shadows on the ground. Whiskers hopped gracefully from stone to stone, while Buddy trotted beside her, his nose sniffing the air for the scent of adventure. They laughed and played, chasing butterflies and exploring nooks and crannies.\n",
      "\n",
      "Suddenly, they stumbled upon a sparkling stream. The water glistened like diamonds, and on the other side, they saw an old, gnarled tree with a door carved into its trunk. ‚ÄúDo you think anyone lives there?‚Äù Buddy whispered, his voice full of wonder.\n",
      "\n",
      "‚ÄúOnly one way to find out!‚Äù Whiskers replied, her eyes gleaming with mischief. With a swift leap, she crossed the stream on a fallen log. Buddy hesitated, looking at the rushing water, then determinedly followed after her, splashing through the shallow parts.\n",
      "\n",
      "When they reached the tree, Whiskers gently nudged the door with her paw. To their surprise, it creaked open, revealing a small room filled with twinkling lights and shelves of colorful jars. In the center stood a wise old owl, perched atop a pile of books.\n",
      "\n",
      "‚ÄúWelcome, young adventurers,‚Äù the owl hooted, adjusting his tiny spectacles. ‚ÄúI see you‚Äôve come seeking magic.‚Äù\n",
      "\n",
      "Whiskers‚Äô eyes widened. ‚ÄúWe want to find treasure and see magical creatures!‚Äù she exclaimed.\n",
      "\n",
      "The owl chuckled softly. ‚ÄúThe greatest treasure lies not in gold or jewels, but in friendship and the adventures you share. But if you truly seek magic, you must prove your bond.‚Äù\n",
      "\n",
      "Intrigued, Buddy and Whiskers nodded eagerly. The owl set forth a series of challenges: they had to work together to solve riddles, navigate a maze of tall grasses, and even help a lost firefly find its way home.\n",
      "\n",
      "With each challenge, they learned more about each other‚Äôs strengths. Whiskers‚Äô agility and cleverness complemented Buddy‚Äôs strength and loyalty. Together, they overcame every obstacle, their laughter echoing in the woods.\n",
      "\n",
      "At last, they returned to the wise owl, who smiled warmly at them. ‚ÄúYou have proven your friendship, and for that, I grant you a gift.‚Äù He waved his wing, and a soft, golden light enveloped them.\n",
      "\n",
      "When the light faded, Whiskers and Buddy found themselves back at the fence, the sun setting behind them. In their paws were two shimmering tokens‚Äîone a small bell, the other a golden bone. \n",
      "\n",
      "‚ÄúEvery time you use these, remember the magic of your friendship and the adventures you can have together,‚Äù the owl‚Äôs voice echoed in their hearts.\n",
      "\n",
      "From that day on, Whiskers and Buddy embarked on countless adventures, exploring the world around them, always together. They discovered that the true magic lay not in treasures or magical creatures, but in the bond they shared‚Äîa friendship that turned every day into an extraordinary adventure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import stdout\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a short story about a cat and a dog.\"}],\n",
    "    temperature=0.7,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    if hasattr(event, \"choices\"):\n",
    "        delta = event.choices[0].delta\n",
    "        if delta and delta.content:\n",
    "            stdout.write(delta.content)\n",
    "stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x1zytae8k1",
   "source": "## 6. JSON and Structured Outputs\n\nExtract structured data from LLM responses using prompt engineering. By instructing the model to format outputs as JSON, you can easily parse and work with the results programmatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5925f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"item\": \"Rice\",\n",
      "        \"quantity\": \"3 kg\"\n",
      "    },\n",
      "    {\n",
      "        \"item\": \"Dhal\",\n",
      "        \"quantity\": \"4 kg\"\n",
      "    },\n",
      "    {\n",
      "        \"item\": \"Biscuits\",\n",
      "        \"quantity\": \"3 packets\"\n",
      "    },\n",
      "    {\n",
      "        \"item\": \"Sugar\",\n",
      "        \"quantity\": \"2 kg\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"I have bought 3 kg of Rice, 4 kg of dhal, 3 packets of biscuits, 2 kg of sugar.\n",
    "\n",
    "Format this as a list of json objects with each JSON object in the following format:\n",
    "{\n",
    "    \"item\": \"item name\",\n",
    "    \"quantity\": \"quantity\"\n",
    "}\n",
    "\n",
    "DO NOT include anything else in your response.\"\"\"\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1c926d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "\n",
    "items = json.loads(response)\n",
    "type(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55e11846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rice\n",
      "Dhal\n",
      "Biscuits\n",
      "Sugar\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    print(item[\"item\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uxf1n2wcf8l",
   "source": "## 7. Pydantic Structured Outputs\n\nUse Pydantic models with `response_format` to get guaranteed structured outputs. OpenAI's API automatically validates and parses responses into type-safe Python objects, eliminating manual parsing and validation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a7efd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary(topic='Transformers in NLP', key_points=['Transformers utilize self-attention mechanisms to weigh the importance of different words in a sentence, allowing for better context understanding.', 'They enable parallel processing of data, significantly improving training efficiency and scalability compared to traditional RNNs and LSTMs.', 'Transformers have led to the development of powerful pre-trained models like BERT and GPT, which can be fine-tuned for various NLP tasks, achieving state-of-the-art results.'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    topic: str\n",
    "    key_points: List[str]\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    response_format=Summary,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Topic: Transformers in NLP. Give 3 key points.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "parsed = completion.choices[0].message.parsed\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "334e5e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Summary"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6d077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items=[ShoppingItem(item='Rice', quantity=3), ShoppingItem(item='Dhal', quantity=4), ShoppingItem(item='Biscuits', quantity=3), ShoppingItem(item='Sugar', quantity=2)]\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"I have bought 3 kg of Rice, 4 kg of dhal, 3 packets of biscuits, 2 kg of sugar.\"\"\"\n",
    "\n",
    "\n",
    "class ShoppingItem(BaseModel):\n",
    "    item: str\n",
    "    quantity: int\n",
    "\n",
    "class ShoppingList(BaseModel):\n",
    "    items: List[ShoppingItem]\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    response_format=ShoppingList,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6b0878d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShoppingList(items=[ShoppingItem(item='Rice', quantity=3), ShoppingItem(item='Dhal', quantity=4), ShoppingItem(item='Biscuits', quantity=3), ShoppingItem(item='Sugar', quantity=2)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3afffb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"items\":[{\"item\":\"Rice\",\"quantity\":3},{\"item\":\"Dhal\",\"quantity\":4},{\"item\":\"Biscuits\",\"quantity\":3},{\"item\":\"Sugar\",\"quantity\":2}]}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}